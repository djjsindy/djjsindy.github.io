<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>djjsindy</title><link href="/" rel="alternate"></link><link href="/feeds/socket.atom.xml" rel="self"></link><id>/</id><updated>2014-05-20T14:51:00+02:00</updated><entry><title>epoll函数原理分析</title><link href="/epollhan-shu-yuan-li-fen-xi.html" rel="alternate"></link><updated>2014-05-20T14:51:00+02:00</updated><author><name>djjsindy</name></author><id>tag:,2014-05-20:epollhan-shu-yuan-li-fen-xi.html</id><summary type="html">&lt;h1&gt;epoll函数原理分析&lt;/h1&gt;
&lt;p&gt;select函数的在监控fd数量非常巨大的情况下，效率低，主要原因：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户态下，每次调用select函数之前，需要把fd关注的事件重新设置，这样select函数到了内核态，需要频繁将用户态的数据复制到内核态。&lt;/li&gt;
&lt;li&gt;设备驱动在唤醒睡眠进程之后，需要重新扫描所有的fd，然后筛选fd产生的事件。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;epoll函数之所以设计的高效是因为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;epoll函数需要创建自己单独的文件系统，返回给用户态epoll fd，这个fd是拿到内核态struct file的关键，找到了这个file，就能找到struct eventpoll全局结构体。每个用户态关注的事件，会直接注册到内核态struct eventpoll里面，所以epoll fd是用户态和内核态struct eventpoll的桥梁。&lt;/li&gt;
&lt;li&gt;有了上面的struct eventpoll，驱动只要把激发的事件放到eventpoll里面的事件就绪队列就可以了，然后唤醒进程后，检查就绪队列是否有事件，如果有直接把激发的事件copy到用户态的events里面即可。不用向select函数那样轮询每个监控的fd了。&lt;/li&gt;
&lt;li&gt;同时内核中对于每个fd形成一个结构体struct epitem，用红黑树来管理，这样保证了插入和查找的效率。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;struct eventpoll结构
&lt;pre&gt;&lt;code&gt;
struct eventpoll {
    /&lt;em&gt; Protect the access to this structure &lt;/em&gt;/
    spinlock_t lock;
    /&lt;em&gt;
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     &lt;/em&gt;/
    struct mutex mtx;
    /&lt;em&gt; Wait queue used by sys_epoll_wait() &lt;/em&gt;/
    //阻塞在epoll_wait函数的阻塞队列，当添加通过epoll_ctl添加事件后，poll函数返回了触发的事件，那么会直接通过这个wq唤醒epoll_wait进程，或者中断处理函数唤醒epoll_wait进程
    wait_queue_head_t wq; &lt;br /&gt;
    /&lt;em&gt; Wait queue used by file-&amp;gt;poll() &lt;/em&gt;/
    //调用epoll fd poll函数的进程的阻塞队列，关心的是是否有注册事件，当有注册事件的时候会唤醒相关进程
    wait_queue_head_t poll_wait;
    /&lt;em&gt; List of ready file descriptors &lt;/em&gt;/
    //就绪事件队列，中断会入队事件
    struct list_head rdllist; 
    /&lt;em&gt; RB tree root used to store monitored fd structs &lt;/em&gt;/
    //管理fd的红黑树
    struct rb_root rbr;
    /&lt;em&gt;
     * This is a single linked list that chains all the "struct epitem" that
     * happened while transferring ready events to userspace w/out
     * holding -&amp;gt;lock.
     &lt;/em&gt;/
    struct epitem &lt;em&gt;ovflist;
    /&lt;/em&gt; The user that created the eventpoll descriptor &lt;em&gt;/
    struct user_struct &lt;/em&gt;user;
};
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;epoll_create函数创建struct file，struct eventpoll，把eventpoll放到file的private_data中，以后使用eventpoll的时候，直接通过fd找到file，再通过private_data找到。&lt;/p&gt;
&lt;p&gt;epoll_ctl注册事件，主要流程设置阻塞队列，调用fd的poll函数，监测是否有事件发生，如果已经有事件发生，那么把事件放到rdllist中，同时唤醒eventpoll-&amp;gt;wq中的进程。&lt;/p&gt;
&lt;p&gt;核心代码：
&lt;pre&gt;&lt;code&gt;
    epi = ep_find(ep, tfile, fd);//通过红黑树来查找fd对应的epitem
    error = -EINVAL;
    switch (op) {
    case EPOLL_CTL_ADD:
        if (!epi) {
            epds.events |= POLLERR | POLLHUP;
            error = ep_insert(ep, &amp;amp;epds, tfile, fd); //添加关注事件
        } else
            error = -EEXIST;
        break;
    case EPOLL_CTL_DEL:
        if (epi)
            error = ep_remove(ep, epi);
        else
            error = -ENOENT;
        break;
    case EPOLL_CTL_MOD:
        if (epi) {
            epds.events |= POLLERR | POLLHUP;
            error = ep_modify(ep, epi, &amp;amp;epds);
        } else
            error = -ENOENT;
        break;
    }
    mutex_unlock(&amp;amp;ep-&amp;gt;mtx);&lt;/p&gt;
&lt;p&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;关注添加关注事件函数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
      //设置poll wait函数，设置wakeup回调函数
     init_poll_funcptr(&amp;epq.pt, ep_ptable_queue_proc);

    /*
     * Attach the item to the poll hooks and get current event bits.
     * We can safely use the file* here because its usage count has
     * been increased by the caller of this function. Note that after
     * this operation completes, the poll callback can start hitting
     * the new item.
     */
    //调用tcp_poll函数，首先回调poll_wait函数，设置阻塞队列在socket的wait queue上
    revents = tfile-&gt;f_op-&gt;poll(tfile, &amp;epq.pt);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;poll wait回调函数是ep_ptable_queue_proc&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//whead是struct sock中的wait queue
static void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,
                 poll_table *pt)
{
    struct epitem *epi = ep_item_from_epqueue(pt);
    struct eppoll_entry *pwq;
     //分配wait entry
    if (epi-&gt;nwait &gt;= 0 &amp;&amp; (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
        //设置wakeup函数
        init_waitqueue_func_entry(&amp;pwq-&gt;wait, ep_poll_callback);
        pwq-&gt;whead = whead;
        pwq-&gt;base = epi;
        //加入等待队列
        add_wait_queue(whead, &amp;pwq-&gt;wait);
        list_add_tail(&amp;pwq-&gt;llink, &amp;epi-&gt;pwqlist);
        epi-&gt;nwait++;
    } else {
        /* We have to signal that an error occurred */
        epi-&gt;nwait = -1;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ep_epoll_callback函数是中断回调函数，和select函数调用机制相同&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
    int pwake = 0;
    unsigned long flags;
    struct epitem *epi = ep_item_from_wait(wait);
    struct eventpoll *ep = epi-&gt;ep;

    spin_lock_irqsave(&amp;ep-&gt;lock, flags);

    /*
     * If the event mask does not contain any poll(2) event, we consider the
     * descriptor to be disabled. This condition is likely the effect of the
     * EPOLLONESHOT bit that disables the descriptor when an event is received,
     * until the next EPOLL_CTL_MOD will be issued.
     */
     //如果设置了EPOLLONESHOT，那么只能触发一次事件
    if (!(epi-&gt;event.events &amp; ~EP_PRIVATE_BITS))
        goto out_unlock;

    /*
     * Check the events coming with the callback. At this stage, not
     * every device reports the events in the "key" parameter of the
     * callback. We need to be able to handle both cases here, hence the
     * test for "key" != NULL before the event match test.
     */
    //如果触发了事件，如果和注册的事件有交集，就继续加入rdllist，否则退出
    if (key &amp;&amp; !((unsigned long) key &amp; epi-&gt;event.events))
        goto out_unlock;

    /*
     * If we are transferring events to userspace, we can hold no locks
     * (because we're accessing user memory, and because of linux f_op-&gt;poll()
     * semantics). All the events that happen during that period of time are
     * chained in ep-&gt;ovflist and requeued later on.
     */
    //在最后copy事件到用户空间的时候会占用rdllist，这时把事件先放到ovflist中
    if (unlikely(ep-&gt;ovflist != EP_UNACTIVE_PTR)) {
        if (epi-&gt;next == EP_UNACTIVE_PTR) {
            epi-&gt;next = ep-&gt;ovflist;
            ep-&gt;ovflist = epi;
        }
        goto out_unlock;
    }
    //把事件加入rdllist中，一个fd一次
    /* If this file is already in the ready list we exit soon */
    if (!ep_is_linked(&amp;epi-&gt;rdllink))
        list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist);

    /*
     * Wake up ( if active ) both the eventpoll wait list and the -&gt;poll()
     * wait list.
     */
    //唤醒epoll_wait中的进程
    if (waitqueue_active(&amp;ep-&gt;wq))
        wake_up_locked(&amp;ep-&gt;wq);

    if (waitqueue_active(&amp;ep-&gt;poll_wait))
        pwake++;

out_unlock:
    spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);

    /* We have to call this outside the lock */
    if (pwake)
        ep_poll_safewake(&amp;ep-&gt;poll_wait);

    return 1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上需要注意的是，当epoll_wait进程正在处理rdllist的时候（把事件copy到用户态的变量），激发的事件会暂时放在ovflist，在说下epoll_wait的流程&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
fetch_events:
    spin_lock_irqsave(&amp;ep-&gt;lock, flags);

    if (!ep_events_available(ep)) {
        /*
         * We don't have any available event to return to the caller.
         * We need to sleep here, and we will be wake up by
         * ep_poll_callback() when events will become available.
         */
        init_waitqueue_entry(&amp;wait, current);  //初始化wait queue
        __add_wait_queue_exclusive(&amp;ep-&gt;wq, &amp;wait); //加入wait queue,中断后满足条件，会唤醒这个进程

        for (;;) {
            /*
             * We don't want to sleep if the ep_poll_callback() sends us
             * a wakeup in between. That's why we set the task state
             * to TASK_INTERRUPTIBLE before doing the checks.
             */
            set_current_state(TASK_INTERRUPTIBLE);
            if (ep_events_available(ep) || timed_out) //检查eventpoll的rdllist是否有激发的事件或者有事件暂时放在ovflist
                break;
            if (signal_pending(current)) { //如果有信号，那么也退出
                res = -EINTR;
                break;
            }

            spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);
            if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) //进程休眠
                timed_out = 1;

            spin_lock_irqsave(&amp;ep-&gt;lock, flags);
        }
        __remove_wait_queue(&amp;ep-&gt;wq, &amp;wait); //已经有事件了，去掉阻塞队列

        set_current_state(TASK_RUNNING);
    }
check_events:
    /* Is it worth to try to dig for events ? */
    eavail = ep_events_available(ep); //是否有事件

    spin_unlock_irqrestore(&amp;ep-&gt;lock, flags);

    /*
     * Try to transfer events to user space. In case we get 0 events and
     * there's still timeout left over, we go trying again in search of
     * more luck.
     */
     //把事件复制到用户态的events
    if (!res &amp;&amp; eavail &amp;&amp;
        !(res = ep_send_events(ep, events, maxevents)) &amp;&amp; !timed_out)
        goto fetch_events;

    return res;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终的收尾工作是ep_send_events，把rdllist中的事件copy到events里面，最终是函数ep_send_events_proc
完成这个工作&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
for (eventcnt = 0, uevent = esed-&gt;events;
         !list_empty(head) &amp;&amp; eventcnt &lt; esed-&gt;maxevents;) {
        epi = list_first_entry(head, struct epitem, rdllink);

        list_del_init(&amp;epi-&gt;rdllink);
        //检查事件，wait是NULL，不会设置阻塞队列
        revents = epi-&gt;ffd.file-&gt;f_op-&gt;poll(epi-&gt;ffd.file, NULL) &amp;
            epi-&gt;event.events;

        /*
         * If the event mask intersect the caller-requested one,
         * deliver the event to userspace. Again, ep_scan_ready_list()
         * is holding "mtx", so no operations coming from userspace
         * can change the item.
         */
        if (revents) {
        //把事件放入用户态变量events
            if (__put_user(revents, &amp;uevent-&gt;events) ||
                __put_user(epi-&gt;event.data, &amp;uevent-&gt;data)) {
                list_add(&amp;epi-&gt;rdllink, head);
                return eventcnt ? eventcnt : -EFAULT;
            }
            eventcnt++;
            uevent++;
            if (epi-&gt;event.events &amp; EPOLLONESHOT)
                epi-&gt;event.events &amp;= EP_PRIVATE_BITS;
            else if (!(epi-&gt;event.events &amp; EPOLLET)) {
              //水平模式
                /*
                 * If this file has been added with Level
                 * Trigger mode, we need to insert back inside
                 * the ready list, so that the next call to
                 * epoll_wait() will check again the events
                 * availability. At this point, no one can insert
                 * into ep-&gt;rdllist besides us. The epoll_ctl()
                 * callers are locked out by
                 * ep_scan_ready_list() holding "mtx" and the
                 * poll callback will queue them in ep-&gt;ovflist.
                 */
                list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist);
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后说下epoll的水平触发和边沿触发模式&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;水平触发：每次触发事件后，无论用户态进程如何用read write处理数据，都会轮询调用fd的poll函数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;边沿触发：在中断处理函数处理后，发现事件，才会触发事件，其他时间不会主动轮询poll函数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以说select函数都是水平触发，调用select函数都是会轮询每个监控fd的poll函数，看是否有事件发生。所以对于用户态进程read write函数，如何处理数据，都无关，即便read取不干净buf中的数据，下次select函数轮询fd的poll函数的时候还会触发事件。&lt;/p&gt;
&lt;p&gt;epoll函数的水平触发，epoll函数的本意不是轮询fd的poll函数，假如中断函数唤醒了epoll_wait进程，把事件copy到了用户态，但是用户态进程，以read为例子，没有拿干净buf中的数据，那么如果之后没有数据到达，那么也就触发不了中断，也就触发不了事件，那么buf中剩余的数据是取不出来的。所以epoll函数默认情况下还是会把当前事件再次放到rdllist中，以便轮询事件。&lt;/p&gt;
&lt;p&gt;epoll函数的边缘触发和水平触发的不同之处在于，epoll函数处理完当前事件后，不会把事件再次加入到rdllist中，也就是不会轮询fd的poll函数，那么这种情况下要求，用户态进程调用read函数把buf中的数据全部copy到用户态，write函数要求把buf写满。也就是说read函数需要不停的read，直到read出来的count小于buf的大小。不停的write出去的数据直到write出去的count小于buf的大小。&lt;/p&gt;
&lt;p&gt;从上面的代码可以看出，确实水平模式事件最后还是会再次加入rdllist中，所以如果epoll使用水平模式第一次触发事件的效率比select函数要高很多，因为中断把事件加入到了rdllist中，并没有轮询其他没有事件的fd的poll函数。后面会把epi加入到rdllist中做轮询，如果在socket未有新事件到达的情况下，一次read write未能处理干净数据的话，epoll会多次轮询，这会导致效率低下。处理干净数据后就不会再次加入到rdllist中了。所以水平模式下的epoll在用户态进程一次处理不完数据的情况下，会导致短暂的轮询，处理完成后解除轮询，效率会有比较大的影响。&lt;/p&gt;
&lt;p&gt;再者边缘模式的epoll，不存在轮询的现象，效率自然很高。但是要求read和write函数对buf处理需要彻底。这对于用户态来说，如何防止其他的连接上的数据不被饿死，代码就会更加复杂一些。&lt;/p&gt;</summary><category term="socket"></category></entry><entry><title>select函数原理分析</title><link href="/selecthan-shu-yuan-li-fen-xi.html" rel="alternate"></link><updated>2014-05-19T16:09:00+02:00</updated><author><name>djjsindy</name></author><id>tag:,2014-05-19:selecthan-shu-yuan-li-fen-xi.html</id><summary type="html">&lt;h1&gt;select函数原理分析&lt;/h1&gt;
&lt;p&gt;select函数大体原理：函数首先会递归每个文件描述符检查事件，如果没有相关的事件，那么就把当前进程添加到wait_queue中，然后改变当前进程的状态，使得进程不会得到时间片，从而使进程休眠；当设备中断，调用相关的驱动的方法，这时会检查wait_queue,然后唤醒相关的进程，这样会导致select进程继续执行，从新检查文件描述符的状态，这时至少有一个事件被激发，这样select函数就会返回了。select函数会顺序检查每个文件描述符的状态，当文件描述符数量十分巨大的情况下，会导致select函数的效率下降。&lt;/p&gt;
&lt;p&gt;select的核心函数是do_select函数，代码我就不copy了，说下里面必要重要的wait_queue的相关操作，初始化函数是&lt;code&gt;poll_initwait&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
void poll_initwait(struct poll_wqueues *pwq)
{
    init_poll_funcptr(&amp;pwq-&gt;pt, __pollwait); //设置wait的时候的处理函数，在需要进程需要休眠的时候调用这个函数
    pwq-&gt;polling_task = current; //设置休眠的进程
    pwq-&gt;triggered = 0;
    pwq-&gt;error = 0;
    pwq-&gt;table = NULL; //初始化为null，后面有自己分配的地方
    pwq-&gt;inline_index = 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;do_select&lt;/code&gt;函数第一次会检查一下文件描述符的状态，同时设置好wait_queue触发的状态。&lt;/p&gt;
&lt;p&gt;file_operations的poll函数是检查一下文件是否有满足的状态，在调用poll函数之前，先设置poll关心的key&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
  f_op = file-&gt;f_op;
  mask = DEFAULT_POLLMASK;
  if (f_op &amp;&amp; f_op-&gt;poll) {
    wait_key_set(wait, in, out, bit);
    mask = (*f_op-&gt;poll)(file, wait);
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;wait_key_set&lt;/code&gt;就是设置poll关心的key&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static inline void wait_key_set(poll_table *wait, unsigned long in,
                unsigned long out, unsigned long bit)
{
    if (wait) {
        wait-&gt;key = POLLEX_SET; //无论设置什么key，都需要关心exception状态
        if (in &amp; bit)
            wait-&gt;key |= POLLIN_SET;  //设置读事件的key
        if (out &amp; bit)
            wait-&gt;key |= POLLOUT_SET; //设置写事件的key
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;f_op-&amp;gt;poll&lt;/code&gt;，考虑网络连接fd，tcp连接，对应&lt;code&gt;tcp_poll&lt;/code&gt;函数，这个函数首先设置wait_queue，然后检查socket状态。首先看下设置wait_queue的函数&lt;code&gt;sock_poll_wait&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static inline void sock_poll_wait(struct file *filp,
        wait_queue_head_t *wait_address, poll_table *p)
{
    if (p &amp;&amp; wait_address) { //这里wait_address是struct sock中的sk_wq的wait_queue_head_t，可以看出个大概，就是把poll_table中的进程加入到sk_wq的wait_queue中，然后中断处理函数，会找到sock中的wait head，挨个唤醒进程
        poll_wait(filp, wait_address, p);
        /*
         * We need to be sure we are in sync with the
         * socket flags modification.
         *
         * This memory barrier is paired in the wq_has_sleeper.
        */
        smp_mb();
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;poll_wait函数调用的就是poll_table中的func函数&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static void __pollwait(struct file *filp, wait_queue_head_t *wait_address,
                poll_table *p)
{
    struct poll_wqueues *pwq = container_of(p, struct poll_wqueues, pt);
    struct poll_table_entry *entry = poll_get_entry(pwq);  //获得entry
    if (!entry)
        return;
    get_file(filp);
    entry-&gt;filp = filp; 
    entry-&gt;wait_address = wait_address;
    entry-&gt;key = p-&gt;key; //设置关心的key，根据key来唤醒进程
    init_waitqueue_func_entry(&amp;entry-&gt;wait, pollwake); //设置唤醒回调函数
    entry-&gt;wait.private = pwq;
    add_wait_queue(wait_address, &amp;entry-&gt;wait);//加入到sock的wait队列中
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在创建&lt;code&gt;struct sock&lt;/code&gt;的时候，函数&lt;code&gt;sock_init_data&lt;/code&gt;设置了几个回调函数&lt;/p&gt;
&lt;p&gt;net/core/sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
    sk-&gt;sk_state_change =   sock_def_wakeup; //连接状态变化
    sk-&gt;sk_data_ready   =   sock_def_readable; // socket有可读数据
    sk-&gt;sk_write_space  =   sock_def_write_space; //socket上可以写入数据
    sk-&gt;sk_error_report =   sock_def_error_report; //socket上有错误
    sk-&gt;sk_destruct     =   sock_def_destruct;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这些回调函数都是底层网络驱动通过中断，先把数据提交到网络层，然后提交到传输层，最后调用回调函数来唤醒休眠的进程。具体的回调过程不是这个主题的重点，先不考虑。&lt;/p&gt;
&lt;p&gt;看下sock_def_readable函数的实现&lt;/p&gt;
&lt;p&gt;net/core/sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static void sock_def_readable(struct sock *sk, int len)
{
    struct socket_wq *wq;

    rcu_read_lock();
    wq = rcu_dereference(sk-&gt;sk_wq); //获得等待队列
    if (wq_has_sleeper(wq))
        wake_up_interruptible_sync_poll(&amp;wq-&gt;wait, POLLIN | POLLPRI |
                        POLLRDNORM | POLLRDBAND); //最后的flag匹配entry-&gt;key
    sk_wake_async(sk, SOCK_WAKE_WAITD, POLL_IN); //异步io，发送信号
    rcu_read_unlock();
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最终wake_up_interruptible_sync_poll函数最终会调用到__wake_up_common&lt;/p&gt;
&lt;p&gt;kernel/sched.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
tatic void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
            int nr_exclusive, int wake_flags, void *key)
{
    wait_queue_t *curr, *next;
    //递归每一个挂在sock等待队列中的每一个元素
    list_for_each_entry_safe(curr, next, &amp;q-&gt;task_list, task_list) {
        unsigned flags = curr-&gt;flags;
        //调用wakeup回调函数
        if (curr-&gt;func(curr, mode, wake_flags, key) &amp;&amp;
                (flags &amp; WQ_FLAG_EXCLUSIVE) &amp;&amp; !--nr_exclusive)
            break;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里func是之前注册的回调函数poll_wake函数&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
static int pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
    struct poll_table_entry *entry;

    entry = container_of(wait, struct poll_table_entry, wait);
    //匹配key，如果匹配才唤醒进程，否则退出
    if (key &amp;&amp; !((unsigned long)key &amp; entry-&gt;key))
        return 0;
    return __pollwake(wait, mode, sync, key);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;tcp_poll函数中设置完wait queue后，会首先检查一下socket的状态，检查是否有相关的事件发生，对于listening状态的socket，只要判断accept队列是否有连接即可，如果不为空，那么触发读相关事件&lt;/p&gt;
&lt;p&gt;首先看下触发读写状态的条件&lt;/p&gt;
&lt;p&gt;fs/select.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//触发读事件
#define POLLIN_SET (POLLRDNORM | POLLRDBAND | POLLIN | POLLHUP | POLLERR)
//触发写事件
#define POLLOUT_SET (POLLWRBAND | POLLWRNORM | POLLOUT | POLLERR)
//触发异常事件
#define POLLEX_SET (POLLPRI)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;
static inline unsigned int inet_csk_listen_poll(const struct sock *sk)
{
    return !reqsk_queue_empty(&amp;inet_csk(sk)-&gt;icsk_accept_queue) ?
            (POLLIN | POLLRDNORM) : 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果不是listening状态的socket，那么需要判断相关的相关参数
&lt;pre&gt;&lt;code&gt;
    //关闭状态，触发读事件
    if (sk-&amp;gt;sk_shutdown == SHUTDOWN_MASK || sk-&amp;gt;sk_state == TCP_CLOSE)
        mask |= POLLHUP;
    //读关闭，触发读事件
    if (sk-&amp;gt;sk_shutdown &amp;amp; RCV_SHUTDOWN)
        mask |= POLLIN | POLLRDNORM | POLLRDHUP;
    //如果当前socket状态不是sync sent或者sync recv
    if ((1 &amp;lt;&amp;lt; sk-&amp;gt;sk_state) &amp;amp; ~(TCPF_SYN_SENT | TCPF_SYN_RECV)) {
        //获得读水位
        int target = sock_rcvlowat(sk, 0, INT_MAX);
        if (tp-&amp;gt;urg_seq == tp-&amp;gt;copied_seq &amp;amp;&amp;amp;
            !sock_flag(sk, SOCK_URGINLINE) &amp;amp;&amp;amp;
            tp-&amp;gt;urg_data)
            target++;
            //如果有数据超过了水位线,触发读事件
        if (tp-&amp;gt;rcv_nxt - tp-&amp;gt;copied_seq &amp;gt;= target)
            mask |= POLLIN | POLLRDNORM;
        //如果不是写半关闭
        if (!(sk-&amp;gt;sk_shutdown &amp;amp; SEND_SHUTDOWN)) {
            //如果有写的空间，触发写事件
            if (sk_stream_wspace(sk) &amp;gt;= sk_stream_min_wspace(sk)) {
                mask |= POLLOUT | POLLWRNORM;
            } else {&lt;br /&gt;
                set_bit(SOCK_ASYNC_NOSPACE,
                    &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags);
                set_bit(SOCK_NOSPACE, &amp;amp;sk-&amp;gt;sk_socket-&amp;gt;flags);
                if (sk_stream_wspace(sk) &amp;gt;= sk_stream_min_wspace(sk))
                    mask |= POLLOUT | POLLWRNORM;
            }
        } else
            //写半关闭触发写事件
            mask |= POLLOUT | POLLWRNORM;
        if (tp-&amp;gt;urg_data &amp;amp; TCP_URG_VALID)
            mask |= POLLPRI;
    }
    /&lt;em&gt; This barrier is coupled with smp_wmb() in tcp_reset() &lt;/em&gt;/
    smp_rmb();
    if (sk-&amp;gt;sk_err)
        mask |= POLLERR;
    return mask;
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;</summary><category term="socket"></category></entry><entry><title>socket绑定端口流程</title><link href="/socketbang-ding-duan-kou-liu-cheng.html" rel="alternate"></link><updated>2014-05-13T11:33:00+02:00</updated><author><name>djjsindy</name></author><id>tag:,2014-05-13:socketbang-ding-duan-kou-liu-cheng.html</id><summary type="html">&lt;h1&gt;socket绑定端口过程&lt;/h1&gt;
&lt;p&gt;绑定端口分为2种，一种是指定端口号，然后内核去判断这个端口号是否可用，另一种是填端口号为0，内核去从端口号范围内选择一个端口。具体过程是在inet_csk_get_port函数中，本文中内核版本2.6.39.14&lt;/p&gt;
&lt;p&gt;先说明一个非常重要的函数inet_csk_bind_conflict，在bind hash找到相同的端口，那么会根据reuseaddr，reuseport和tcp状态来决定这个端口是否是可用。
&lt;pre&gt;&lt;code&gt;
inet_get_local_port_range(&amp;amp;low, &amp;amp;high); //首先确定端口号范围
        remaining = (high - low) + 1;    //表示可以尝试端口号的机会
        smallest_rover = rover = net_random() % remaining + low;  //从端口号范围内随机选择一个端口号为起点，开始遍历选择&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    &lt;span class="n"&gt;smallest_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  &lt;span class="c1"&gt;//如果没有可用的端口号，会选择被reuse最少的端口号&lt;/span&gt;
    &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inet_is_reserved_local_port&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rover&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="c1"&gt;//如果这个随机端口号是保留的，那么不考虑&lt;/span&gt;
            &lt;span class="k"&gt;goto&lt;/span&gt; &lt;span class="n"&gt;next_nolock&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="c1"&gt;//这里bhash是绑定端口号的hash结构，net是网络命名空间，暂时不考虑，rover是端口参考值，这里计算参考值的hash index，struct inet_bind_bucket是hash的元素，这里取hash位置的双链表的head指针&lt;/span&gt;
        &lt;span class="n"&gt;head&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;hashinfo&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;bhash&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;inet_bhashfn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rover&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;hashinfo&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;bhash_size&lt;/span&gt;&lt;span class="p"&gt;)];&lt;/span&gt; 
        &lt;span class="n"&gt;spin_lock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;lock&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="c1"&gt;//遍历双链表，寻找rover的记录，如果没有找到，那就是可用的端口，如果找到了，判断状态来决定端口是否可用&lt;/span&gt;
        &lt;span class="n"&gt;inet_bind_bucket_for_each&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net_eq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ib_net&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;port&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;rover&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;//如果已经绑定的socket设置了reuse，同时当前socket设置了reuse，当前socket状态不是listen状态，同时这个端口的可重用的阈值是最小的，或重用阈值为-1（初始阈值），目的是找到端口重用度最小的阈值的那个端口&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;fastreuse&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
                    &lt;span class="n"&gt;sk&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sk_reuse&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
                    &lt;span class="n"&gt;sk&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;sk_state&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;TCP_LISTEN&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;num_owners&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;smallest_size&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;smallest_size&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="c1"&gt;//更新阈值&lt;/span&gt;
                    &lt;span class="n"&gt;smallest_size&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tb&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;num_owners&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                    &lt;span class="n"&gt;smallest_rover&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rover&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="c1"&gt;//保存这个最小阈值的端口&lt;/span&gt;
                    &lt;span class="c1"&gt;//如果端口bind hash已经有了足够多的绑定端口，那么当前端口就是备选。避免了遍历数量很多的bind hash，这个应该是内核中的一个小小的优化吧。以前的版本没看过&lt;/span&gt;
                    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;atomic_read&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;hashinfo&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;bsockets&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;high&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;low&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                        &lt;span class="n"&gt;spin_unlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;lock&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                        &lt;span class="n"&gt;snum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;smallest_rover&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                        &lt;span class="k"&gt;goto&lt;/span&gt; &lt;span class="n"&gt;have_snum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                    &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="k"&gt;goto&lt;/span&gt; &lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="c1"&gt;//到了这里表示端口已经找到并且这个端口&lt;/span&gt;
        &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nl"&gt;next:&lt;/span&gt;
        &lt;span class="n"&gt;spin_unlock&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;lock&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nl"&gt;next_nolock:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="n"&gt;rover&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;high&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;rover&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;low&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;remaining&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

    &lt;span class="cm"&gt;/* Exhausted local port range during search?  It is not&lt;/span&gt;
&lt;span class="cm"&gt;     * possible for us to be holding one of the bind hash&lt;/span&gt;
&lt;span class="cm"&gt;     * locks if this test triggers, because if &amp;#39;remaining&amp;#39;&lt;/span&gt;
&lt;span class="cm"&gt;     * drops to zero, we broke out of the do/while loop at&lt;/span&gt;
&lt;span class="cm"&gt;     * the top level, not from the &amp;#39;break;&amp;#39; statement.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
     &lt;span class="c1"&gt;//已经遍历了一遍了，如果阈值（smallest_rover）还是-1，那么证明所有端口都被占用，同时都为设置reuse，那么只能失败了，否则使用reuse占用最少的端口，这种情况也是端口都被占用，但是有一些是reuse的，并且当前的端口是reuse的。&lt;/span&gt;
    &lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;remaining&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;smallest_size&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;snum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;smallest_rover&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="k"&gt;goto&lt;/span&gt; &lt;span class="n"&gt;have_snum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;goto&lt;/span&gt; &lt;span class="n"&gt;fail&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="cm"&gt;/* OK, here is the one we will use.  HEAD is&lt;/span&gt;
&lt;span class="cm"&gt;     * non-NULL and we hold it&amp;#39;s mutex.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="n"&gt;snum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rover&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;1.未指定端口号&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
        //首先确定端口号范围
        inet_get_local_port_range(net, &amp;low, &amp;high);
        //表示可以尝试端口号的机会
        remaining = (high - low) + 1;
        //从端口号范围内随机选择一个端口号为起点，开始遍历选择
        smallest_rover = rover = prandom_u32() % remaining + low;
        //如果没有可用的端口号，会选择被reuse最少的端口号
        smallest_size = -1;
        do {
            //如果这个随机端口号是保留的，那么不考虑
            if (inet_is_reserved_local_port(rover))
                goto next_nolock;
            //这里bhash是绑定端口号的hash结构，net是网络命名空间，暂时不考虑，rover是端口参考值，这里计算参考值的hash index，struct inet_bind_bucket是hash的元素，这里取hash位置的双链表的head指针
            head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, rover,
                    hashinfo-&gt;bhash_size)];
            spin_lock(&amp;head-&gt;lock);
            //遍历双链表，寻找rover的记录，如果没有找到，那就是可用的端口，如果找到了，判断状态来决定端口是否可用
            inet_bind_bucket_for_each(tb, &amp;head-&gt;chain)
                //如果已经绑定的端口在同一命名空间中，端口是当前随机端口
                if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == rover) {
                    //如果tb和sk同时设置了reuse，sk状态不是listen或者reuseport，同时这个端口的重用阈值未设置，或者小于阈值，那么这个端口作为备选
                    if (((tb-&gt;fastreuse &gt; 0 &amp;&amp; sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN)||(tb-&gt;fastreuseport &gt; 0 &amp;&amp;sk-&gt;sk_reuseport &amp;&amp;uid_eq(tb-&gt;fastuid, uid))) &amp;&amp;
                        (tb-&gt;num_owners &lt; smallest_size || smallest_size == -1)) {
                        smallest_size = tb-&gt;num_owners;
                        smallest_rover = rover;
                        //如果hashinfo中的bind到了一定的数量，就挑选当前重用度最低的端口
                        if (atomic_read(&amp;hashinfo-&gt;bsockets) &gt; (high - low) + 1 &amp;&amp;
                            !inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) {
                            snum = smallest_rover;
                            goto tb_found;
                        }
                    }
                    if (!inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb, false)) {
                        snum = rover;
                        goto tb_found;
                    }
                    goto next;
                }
            //找到了未绑定的端口就退出
            break;
        next:
            spin_unlock(&amp;head-&gt;lock);
        next_nolock:
            if (++rover &gt; high)
                rover = low;
        } while (--remaining &gt; 0);

        /* Exhausted local port range during search?  It is not
         * possible for us to be holding one of the bind hash
         * locks if this test triggers, because if 'remaining'
         * drops to zero, we broke out of the do/while loop at
         * the top level, not from the 'break;' statement.
         */
        ret = 1;
        if (remaining &lt;= 0) {
            if (smallest_size != -1) {
                snum = smallest_rover;
                goto have_snum;
            }
            goto fail;
        }
        /* OK, here is the one we will use.  HEAD is
         * non-NULL and we hold it's mutex.
         */
        snum = rover;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.指定了端口号&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
//如果指定了端口号
 else {
have_snum:
        head = &amp;hashinfo-&gt;bhash[inet_bhashfn(net, snum,
                hashinfo-&gt;bhash_size)];
        spin_lock(&amp;head-&gt;lock);
        //如果在同一个命名空间，端口号相同就去tb_found,否则就是去tb_not_found
        inet_bind_bucket_for_each(tb, node, &amp;head-&gt;chain)
            if (net_eq(ib_net(tb), net) &amp;&amp; tb-&gt;port == snum)
                goto tb_found;
    }
    tb = NULL;
    goto tb_not_found;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从上面的代码中可以看出，如果未指定端口，如果从bind hash找到了未使用的端口就使用，否则表示所有端口都已经使用，同时已经选择除了重用度最小的端口作为备选，这种情况会到tb_found。再有如果指定了端口，同时这个端口已经被占用，也会到tb_found。&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
tb_found:
    //如果reuse的使用者不为空
    if (!hlist_empty(&amp;tb-&gt;owners)) {
        //如果占用端口的reuse为1，同时当前reuse也为1，socket状态不是listen，
        smallest_size为-1表示指定了端口，端口被占用。未指定的端口也可能到这里，就是bind hash已经超过了(high - low) + 1
        if (tb-&gt;fastreuse &gt; 0 &amp;&amp;
            sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN &amp;&amp;
            smallest_size == -1) {
            goto success;
        } else {
            //这里要调用bind_conflict函数，如果失败，socket是reuse，attempts&gt;0,表示还有机会选择其他的端口
            ret = 1;
            if (inet_csk(sk)-&gt;icsk_af_ops-&gt;bind_conflict(sk, tb)) {
                if (sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN &amp;&amp;
                    smallest_size != -1 &amp;&amp; --attempts &gt;= 0) {
                    spin_unlock(&amp;head-&gt;lock);
                    goto again;
                }
                goto fail_unlock;
            }
        }
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;bind_conflict函数的实现&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
int inet_csk_bind_conflict(const struct sock *sk,
               const struct inet_bind_bucket *tb)
{
    struct sock *sk2;
    struct hlist_node *node;
    int reuse = sk-&gt;sk_reuse;

    /*
     * Unlike other sk lookup places we do not check
     * for sk_net here, since _all_ the socks listed
     * in tb-&gt;owners list belong to the same net - the
     * one this bucket belongs to.
     */
    //对于reuse的端口进行便利
    //不使用同一个接收地址的socket可以共用端口号，绑定在不同的网络设备接口上的socket可以共用端口号，或者两个socket都表示自己可以被重用，并且还不在TCP_LISTEN状态，则可以重用端口号。
    sk_for_each_bound(sk2, node, &amp;tb-&gt;owners) {
        if (sk != sk2 &amp;&amp;
            !inet_v6_ipv6only(sk2) &amp;&amp;
            (!sk-&gt;sk_bound_dev_if ||
             !sk2-&gt;sk_bound_dev_if ||
             sk-&gt;sk_bound_dev_if == sk2-&gt;sk_bound_dev_if)) {
            if (!reuse || !sk2-&gt;sk_reuse ||
                sk2-&gt;sk_state == TCP_LISTEN) {
                const __be32 sk2_rcv_saddr = sk_rcv_saddr(sk2);
                if (!sk2_rcv_saddr || !sk_rcv_saddr(sk) ||
                    sk2_rcv_saddr == sk_rcv_saddr(sk))
                    break;
            }
        }
    }
    return node != NULL;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;后面的逻辑是如果这个端口不是重用端口，那么就在bind hash中创建记录&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
ret = 1;
    //如果在hash中有记录，那么tb一定是reuse的端口，否则在hash中创建记录
    if (!tb &amp;&amp; (tb = inet_bind_bucket_create(hashinfo-&gt;bind_bucket_cachep,
                    net, head, snum)) == NULL)
        goto fail_unlock;
    //如果这个端口没有重用过，新建立的端口
    if (hlist_empty(&amp;tb-&gt;owners)) {
        if (sk-&gt;sk_reuse &amp;&amp; sk-&gt;sk_state != TCP_LISTEN)
        //reuse置为1
            tb-&gt;fastreuse = 1;
        else
            tb-&gt;fastreuse = 0;//表示不能重用
    //这个暂时没有搞懂的逻辑
    } else if (tb-&gt;fastreuse &amp;&amp;
           (!sk-&gt;sk_reuse || sk-&gt;sk_state == TCP_LISTEN))
        tb-&gt;fastreuse = 0;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后做收尾工作&lt;/p&gt;
&lt;p&gt;net/ipv4/inet_connection_sock.c&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
void inet_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,
            const unsigned short snum)
{
    struct inet_hashinfo *hashinfo = sk-&gt;sk_prot-&gt;h.hashinfo;

    atomic_inc(&amp;hashinfo-&gt;bsockets);

    inet_sk(sk)-&gt;inet_num = snum;
    sk_add_bind_node(sk, &amp;tb-&gt;owners);
    tb-&gt;num_owners++;
    inet_csk(sk)-&gt;icsk_bind_hash = tb;
}
&lt;/code&gt;&lt;/pre&gt;</summary><category term="socket"></category></entry><entry><title>socket创建流程2</title><link href="/socketchuang-jian-liu-cheng-2.html" rel="alternate"></link><updated>2014-05-09T15:14:00+02:00</updated><author><name>djjsindy</name></author><id>tag:,2014-05-09:socketchuang-jian-liu-cheng-2.html</id><summary type="html">&lt;h1&gt;socket创建流程2&lt;/h1&gt;
&lt;p&gt;linux内核创建socket时，会初始化&lt;code&gt;struct sock&lt;/code&gt;，后面针对net family的不通和protocol的不同，对&lt;code&gt;struct sock&lt;/code&gt;有多种扩展，主要的扩展图如下：&lt;/p&gt;</summary><category term="socket"></category></entry><entry><title>socket创建流程1</title><link href="/socketchuang-jian-liu-cheng-1.html" rel="alternate"></link><updated>2014-05-09T10:49:00+02:00</updated><author><name>djjsindy</name></author><id>tag:,2014-05-09:socketchuang-jian-liu-cheng-1.html</id><summary type="html">&lt;h1&gt;socket创建流程1&lt;/h1&gt;
&lt;p&gt;linux内核在系统初始化的过程中，会对支持的多个协议组进行初始化，例如ipv4，ipv6，unix域协议。这些协议的结构体通过&lt;code&gt;sock_register&lt;/code&gt;放在一个&lt;code&gt;net_families&lt;/code&gt;的数组中。&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lh5.googleusercontent.com/-fdHJThfthJ0/U2tXrsCfNjI/AAAAAAAAAPk/xtrusNqcVis/s1024/Screen%2520Shot%25202014-05-08%2520at%25206.07.45%2520PM.png" width="600"&gt;&lt;/p&gt;
&lt;p&gt;linux内核在初始化过程中，会调用inet_init函数，前半部分重要的代码：&lt;/p&gt;
&lt;p&gt;net/ipv4/af_inet.c&lt;/p&gt;
&lt;p&gt;&lt;pre&gt;&lt;code&gt;&lt;br /&gt;
    (void)sock_register(&amp;amp;inet_family_ops);  //该回调函数注册inet_family_ops
    #ifdef CONFIG_SYSCTL
    ip_static_sysctl_init();      &lt;br /&gt;
    #endif   //以下注册网络层到  &lt;br /&gt;
    if (inet_add_protocol(&amp;amp;icmp_protocol, IPPROTO_ICMP) &amp;lt; 0)
        printk(KERN_CRIT "inet_init: Cannot add ICMP protocol\n");
    if (inet_add_protocol(&amp;amp;udp_protocol, IPPROTO_UDP) &amp;lt; 0)
        printk(KERN_CRIT "inet_init: Cannot add UDP protocol\n");
    if (inet_add_protocol(&amp;amp;tcp_protocol, IPPROTO_TCP) &amp;lt; 0)
        printk(KERN_CRIT "inet_init: Cannot add TCP protocol\n");
    #ifdef CONFIG_IP_MULTICAST
    if (inet_add_protocol(&amp;amp;igmp_protocol, IPPROTO_IGMP) &amp;lt; 0)
        printk(KERN_CRIT "inet_init: Cannot add IGMP protocol\n");
    #endif
    /&lt;em&gt; Register the socket-side information for inet_create. &lt;/em&gt;/ 
    for (r = &amp;amp;inetsw[0]; r &amp;lt; &amp;amp;inetsw[SOCK_MAX]; ++r)
        INIT_LIST_HEAD(r);
    for (q = inetsw_array; q &amp;lt; &amp;amp;inetsw_array[INETSW_ARRAY_LEN]; ++q)
        inet_register_protosw(q);  //注册传输层协议结构体&lt;/p&gt;
&lt;p&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;上面的代码中&lt;code&gt;inetsw_array&lt;/code&gt;是一个表示传输层协议的结构体
net/ipv4/af_inet.c
&lt;pre&gt;&lt;code&gt;
static struct inet_protosw inetsw_array[] =
{
    {
        .type =       SOCK_STREAM,   //字节流
        .protocol =   IPPROTO_TCP, &lt;br /&gt;
        .prot =       &amp;amp;tcp_prot,     //tcp的一些回调函数
        .ops =        &amp;amp;inet_stream_ops,  //字节流模式的回调函数
        .no_check =   0,
        .flags =      INET_PROTOSW_PERMANENT |INET_PROTOSW_ICSK,
    },
    {
        .type =       SOCK_DGRAM,
        .protocol =   IPPROTO_UDP,
        .prot =       &amp;amp;udp_prot,
        .ops =        &amp;amp;inet_dgram_ops,
        .no_check =   UDP_CSUM_DEFAULT,
        .flags =      INET_PROTOSW_PERMANENT,
    },
    {
        .type =       SOCK_RAW,
        .protocol =   IPPROTO_IP,   /&lt;em&gt; wild card &lt;/em&gt;/
        .prot =       &amp;amp;raw_prot,
        .ops =        &amp;amp;inet_sockraw_ops,
        .no_check =   UDP_CSUM_DEFAULT,
        .flags =      INET_PROTOSW_REUSE,
    }
};
&lt;/pre&gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;所以对于对于经常写的代码&lt;code&gt;socket(AF_INET, SOCK_STREAM, 0);&lt;/code&gt;，创建socket结构，首先根据第一个参数&lt;code&gt;AF_INET&lt;/code&gt;，在&lt;code&gt;net_families&lt;/code&gt;数组中匹配到&lt;code&gt;inet_family_ops&lt;/code&gt;，再根据&lt;code&gt;inetsw_array&lt;/code&gt;数组中的type和protocol设置prot和ops指针，&lt;code&gt;inet_init&lt;/code&gt;后面的初始化暂时看不懂。&lt;/p&gt;
&lt;p&gt;net/socket.c
&lt;pre&gt;&lt;code&gt;
pf = rcu_dereference(net_families[family]); 
//根据family确定使用哪个family，AF_INET使用inet_family_ops
err = -EAFNOSUPPORT;
if (!pf)
    goto out_release;
if (!try_module_get(pf-&amp;gt;owner))
    goto out_release;
rcu_read_unlock();
err = pf-&amp;gt;create(net, sock, protocol, kern); //调用inet的create函数
if (err &amp;lt; 0)
    goto out_module_put;&lt;/p&gt;
&lt;p&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;net/ipv4/af_inet.c
&lt;pre&gt;&lt;code&gt;
//根据type和protocol从inetsw_array选择合适的inet_protosw
list_for_each_entry_rcu(answer, &amp;amp;inetsw[sock-&amp;gt;type], list) {
        err = 0;
        if (protocol == answer-&amp;gt;protocol) {
            if (protocol != IPPROTO_IP)
                break;
        } else {
            /&lt;em&gt; Check for the two wild cases. &lt;/em&gt;/
            if (IPPROTO_IP == protocol) {
                protocol = answer-&amp;gt;protocol;
                break;
            }
            if (IPPROTO_IP == answer-&amp;gt;protocol)
                break;
        }
        err = -EPROTONOSUPPORT;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/p&gt;
&lt;p&gt;看下struct socket的结构&lt;/p&gt;
&lt;p&gt;&lt;img src="https://lh6.googleusercontent.com/-ntSul2-gNTc/U2w7Sm1rPMI/AAAAAAAAAQE/OeK6SvuhXmU/s800/Screen%2520Shot%25202014-05-09%2520at%252010.16.33%2520AM.png" width="500"&gt;&lt;/p&gt;
&lt;p&gt;可以看出struct socket的结构分为了2个部分，通过file连接底层驱动，sock记录tcp连接状态，proto_ops是各种操作函数的结构体，上面的代码选择出了protocol，那么对应到这图中，对struct socket中的proto_ops和sock_common中的skc_prot进行了赋值。这两个都是相关协议的回调函数接口。&lt;/p&gt;</summary><category term="socket"></category></entry></feed>